{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¼ Pandas (pd)"
      ],
      "metadata": {
        "id": "MtO1z0WdGzyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pandas** is a Python library used to work with structured data, especially tabular data like spreadsheets or databases. It provides two main data structures:\n",
        "* **Series**: a one-dimensional labeled array (like a column).\n",
        "* **DataFrame**: a two-dimensional labeled table (like an Excel spreadsheet).\n",
        "\n",
        "Pandas has many features that make it advantageous for **data analysis**: (1) Pandas can easily read data from files (e.g., `.csv`, `.xlsx`) into Python with functions like `pd.read_csv()` and `pd.read_excel()`; (2) DataFrames are designed to be intuitive, looking like tables that you would see in Excel or Google Sheets; (3) Pandas can easily perform a variety of data operations (e.g., filter rows, compute statistics, sort, group by category), and (4) Pandas works well with NumPy and Matplotlib; Pandas is fast and efficient, especially when dealing with large datasets.\n",
        "\n",
        "To import Pandas, we only have to run the following code:"
      ],
      "metadata": {
        "id": "YPuR71dNIuP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Pandas\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "IF59neuDMBb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating/Loading Your DataFrame"
      ],
      "metadata": {
        "id": "8cdyOCQiHHwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From a Python Dictionary**\n",
        "\n",
        "One of the easiest ways to create a DataFrame is from a Python dictionary, where each key becomes a column name and each list becomes the column value. This is achieved by the `pd.DataFrame(data_dict)` function, where `data_dict` is the Python dictionary."
      ],
      "metadata": {
        "id": "duNFMna1HUla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\n",
        "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
        "    \"Age\": [16, 17, 18],\n",
        "    \"Grade\": [\"A\", \"B\", \"A\"]\n",
        "}\n",
        "\n",
        "df_dict = pd.DataFrame(data_dict)\n",
        "#print(df_dict)\n",
        "df_dict"
      ],
      "metadata": {
        "id": "ZGX8ZrcHG9nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From a NumPy Array**\n",
        "\n",
        "If you're already working with NumPy arrays, you can also convert these into DataFrames by specifying column names. This is achieved by the `pd.DataFrame(data_array,columns)` function, where `data_array` is the NumPy array and `columns = [Col_Name1, Col_Name2]` is a list containing the names of each column of the `data_array`."
      ],
      "metadata": {
        "id": "jaDkqReaNLWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data_array = np.array([[81, 94], [92, 77], [79, 86]])\n",
        "df_array = pd.DataFrame(data_array, columns=[\"Math Grade\", \"Science Grade\"])\n",
        "#print(df_array)\n",
        "\n",
        "df_array"
      ],
      "metadata": {
        "id": "hpg59_GRNTzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From a CSV File**\n",
        "\n",
        "Real-world data is often stored in files like CSVs (Comma-Separated Values). Pandas makes it easy to read these files into DataFrames using the `pd.read_csv()` function that can read a CSV file whether its hosted locally on your computer or online."
      ],
      "metadata": {
        "id": "-yaBe5iDNuo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading a Local CSV File\n",
        "* Input the `filepath` of where the CSV file is located into the function `pd.read_csv()`.\n",
        "* The easiest way to know where your CSV file is by uploading the file into your Google Colab `content` folder:\n",
        "  * Click the ðŸ“ icon on the vertical toolbar on the left-hand side of the window.\n",
        "  * Although it doesn't say, this default folder location is your `content` folder.\n",
        "  * Drag and drop your CSV file into this folder.\n",
        "  * The filepath for your CSV file (let's call it `myfile.csv`) is `\"/content/myfile.csv\"`\n",
        "\n",
        "ðŸ¤ **Example**: Load Population Data with a Local CSV File\n",
        "1. Clicking this [link](https://raw.githubusercontent.com/cdefinnda/Python-Intro/main/Datasets/Population.csv) to open the CSV file in your browser. Note that this dataset is from [Our World in Data](https://ourworldindata.org/population-growth).\n",
        "2. Right click anywhere on that page and the select `Save As...`.\n",
        "3. Download the CSV file onto your computer and name it `pop_data.csv`.\n",
        "4. Click the ðŸ“ icon on the vertical toolbar to the left (this is your `content` folder).\n",
        "5. Drag and drop `pop_data.csv` into your local `content` folder.\n",
        "6. Run the code below to read this CSV file as a DataFrame."
      ],
      "metadata": {
        "id": "nKlCPyQXRwZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_local = pd.read_csv(\"/content/Population.csv\")\n",
        "\n",
        "df_local"
      ],
      "metadata": {
        "id": "NGqykwwsS6zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading an Online CSV File:\n",
        "* Input the `url` of where the .csv file is hosted online into the function `pd.read_csv()`.\n",
        "\n",
        "ðŸ¤ **Example**: Load Population Data with an Online CSV File\n",
        "1. If we have a [link](https://raw.githubusercontent.com/cdefinnda/intro/main/population-with-un-projections.csv) to the CSV file, we can use the `url` directly.\n",
        "2. Run the code below to read this CSV file as a DataFrame."
      ],
      "metadata": {
        "id": "IzlvCUEzRpzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/cdefinnda/Python-Intro/main/Datasets/Population.csv\"\n",
        "df_online = pd.read_csv(url)\n",
        "\n",
        "df_online"
      ],
      "metadata": {
        "id": "AaaqDXb8REQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also control how your CSV file is loaded into a DataFrame by inputting additional arguments into `pd.read_csv()`. Here are some examples:\n",
        "* `pd.read_csv(\"data.csv\", sep=\";\")`: Use a semicolon as the delimiter instead of a comma.\n",
        "* `pd.read_csv(\"data.tsv\", sep=\"\\t\")`: Load a tab-separated file (TSV)\n",
        "* `pd.read_csv(\"data.csv\", usecols=[\"Col_Name1\",\"Col_Name2\"])`: Load only specific columns. The column names must match what's in the the CSV file.\n",
        "* `pd.read_csv(\"data.csv\", skiprows=2)`: Skip the first two rows of the file.\n",
        "* `pd.read_csv(\"data.csv\", header=None)`: Treat the file as having no header row.\n",
        "* `pd.read_csv(\"data.csv\", header=None, names=[\"Col_Name1\",\"Col_Name2\"])`: Assign custome column names.\n",
        "* `pd.read_csv(\"data.csv\", index_col=\"ID\")`: Set a specific column as the index.\n",
        "* `pd.read_csv(\"data.csv\", na_values=[\"N/A\", \"?\"])`: Mark certain values as missing (NaN).\n",
        "* `pd.read_csv(\"data.csv\", parse_dates=[\"Date\"])`: Parse date strings into datetime objects.\n",
        "* `pd.read_csv(\"data.csv\", nrows=100)`: Load only the first 100 rows."
      ],
      "metadata": {
        "id": "x0UrG8pTZz0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From an Excel File**\n",
        "\n",
        "The function `pd.read_excel()` operates very similarly to `pd.read_csv()`, but with some minor differences to take account for the additional functionality of `.xlsx` files. As CSV files take up less storage than Excel files, you are more likley to load an Excel file from your local computer. In which case you would follow these steps:\n",
        "\n",
        "* Input the `filepath` of where the Excel file is located into the function `pd.read_excel()`.\n",
        "* The easiest way to know where your Excel file is by uploading the file into your Google Colab `content` folder:\n",
        "  * Click the ðŸ“ icon on the vertical toolbar on the left-hand side of the window.\n",
        "  * Although it doesn't say, this default folder location is your `content` folder.\n",
        "  * Drag and drop your Excel file into this folder.\n",
        "  * The filepath for your Excel file (let's call it `myfile.xlsx`) is `\"/content/myfile.xlsx\"`\n",
        "\n",
        "ðŸ¤ **Example**: Load Population Data with a Local Excel File\n",
        "1. Clicking this [link](https://raw.githubusercontent.com/cdefinnda/Python-Intro/main/Datasets/Population.xlsx) to open the Excel file in your browser. Note that this dataset is from [Our World in Data](https://ourworldindata.org/population-growth).\n",
        "2. Right click anywhere on that page and the select `Save As...`.\n",
        "3. Download the Excel file onto your computer and name it `Population.xlsx`.\n",
        "4. Click the ðŸ“ icon on the vertical toolbar to the left (this is your `content` folder).\n",
        "5. Drag and drop `Population.xlsx` into your local `content` folder.\n",
        "6. Run the code below to read this Excel file as a DataFrame."
      ],
      "metadata": {
        "id": "le6GqqQTd_RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_excel = pd.read_excel(\"/content/Population.xlsx\")\n",
        "\n",
        "df_excel"
      ],
      "metadata": {
        "id": "mdYWwdkOe9dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to `pd.read_csv()`, `pd.read_excel()` interprets many of the same arguments (e.g., `usecols=[\"Col_Name1\",\"Col_Name2\"]`, `header=None`, `na_values=[\"-\",\"N/A\"]`). The unique arguments for `pd.read_excel()` typically involve identifying different sheets to read data from:\n",
        "* `pd.read_excel(\"data.xlsx\", sheet_name=\"Sheet1\")`: Load a specific sheet by name. This sheet name must match the sheet name in the `.xlsx` file.\n",
        "* `pd.read_excel(\"data.xlsx\", sheet_name=0)`: Load a specific sheet by index (`0` = first sheet).\n"
      ],
      "metadata": {
        "id": "2_6EKeXKhPad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copying/Saving Your DataFrame"
      ],
      "metadata": {
        "id": "fZH0Gai_4Mvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Copying/Saving Your DataFrame**\n",
        "\n",
        "Once you have loaded your DataFrame, you may want to make changes to it. It is good practice to not make changes to your originally loaded DataFrame, but make changes to a copy of this DataFrame. After you have finished making your changes, you may want to export this modified DataFrame as a CSV or Excel file. To accomplish these tasks, you can use the following commands:\n",
        "\n",
        "* `df_copy = df.copy()`: Creates a completely independent copy of the dataframe `df` and saves it as `df_copy`. Any changes made to the new dataframe will not affect the original.\n",
        "* `df.to_csv(\"my_data.csv\", index=False)`: Saves the dataframe `df` as a CSV file. `index=False` prevents the row index from being saved as an extra column. The default location for saving will be in Google Colab's `/content/` folder.\n",
        "* `df.to_excel(\"my_data.xlsc\", index=False)`: Saves the dataframe `df` as a Excel file. `index=False` prevents the row index from being saved as an extra column. The default location for saving will be in Google Colab's `/content/` folder."
      ],
      "metadata": {
        "id": "3Vdp3EgU5FNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ’ª Practice: Download Another Dataset\n",
        "\n",
        "Load [this dataset](https://raw.githubusercontent.com/cdefinnda/Python-Intro/main/Datasets/Internet.csv) on the share of population using internet into a DataFrame.\n",
        "* Load the dataset either by downloading the CVS file or loading directly using the URL.\n",
        "* When you load this dataset, only load the following columns: `\"Entity\"`, `\"Year\"`, and `\"Individuals using the Internet (% of population)\"`"
      ],
      "metadata": {
        "id": "bHcMRsoTRuEC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yq-T5eyYSNBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring your DataFrame"
      ],
      "metadata": {
        "id": "MfiVfnI5HNRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by loading a modified version of the population dataset from [Our World in Data](https://ourworldindata.org/population-growth):"
      ],
      "metadata": {
        "id": "TZsqRNs99utH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/cdefinnda/Python-Intro/main/Datasets/Population_incomplete_US.csv\"\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "wu43lzpy-GSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding the Structure of the Data"
      ],
      "metadata": {
        "id": "CwmMOZEK-d75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default viewer for dataframes in Google Colab is actually pretty good on its own. To view our dataframe, all we need to do is type `df` as the final command line in your coding cell."
      ],
      "metadata": {
        "id": "EtycpWo09jMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "nWPHIYclS4Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll notice that Google Colab has buttons (see to the upper-right part of the table above) that can help you explore your data:\n",
        "* The `Table` icon converts the dataframe into an interactive table that gives you options to scroll through and filter you data (note that this option may not be available for very large datasets).\n",
        "* The `Graph` icon (ðŸ“Š) suggest charts that to allow you to visualize your data. However, Google Colab is guessing how to visualize your data, which may be difficult for it to do depending on how the data is organized.\n",
        "* The `Pen` icon (ðŸ–‹ï¸) helps you generate code to explore your dataframe. This can be useful, especially when you are wanting to visualize your data in a particular way. However, before you can do this effectively, you first need to understand how your data is structured."
      ],
      "metadata": {
        "id": "_ES-7SzQTxXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the Google Colab buttons, there are some attributes/functions and Pandas that can help you get an initial sense for the structure of your dataframe:\n",
        "* `df.head(n)`: View the first `n` rows (default is `n = 5`).\n",
        "* `df.tail(n)`: View the last `n` rows (default is `n = 5`).\n",
        "* `df.shape`: Returns a tuple `(rows, columns)` â€” shows the size of the DataFrame.\n",
        "* `df.columns`: Lists the column names.\n",
        "* `df[\"Col_Name\"].unique()`: Returns an array with all the unique row values from a specific column (e.g., `\"Col_Name\"`).\n",
        "\n",
        "In addition to these attributes/functions, there are many others that may be more useful the deeper you get into data science. Some of these include: `df.index` (shows the index labels, e.g., 0, 1, 2... by default), `df.dtype` (shows the data type for each column), `df.info` (provides a summary that includes columns, non-null counts, data types, and memory usage), and `df.describe()` (generates a summary of descriptive statistics for each numeric column in your DataFrame, including: `count`; `mean`; `std`; `min`; `25%`, `50%`, `75%` percentiles; and `max`)."
      ],
      "metadata": {
        "id": "LJ9AHht-aI8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ðŸ’ª Practice: Identify Unique Countries in our Dataset\n",
        "Use some of the attributes/functions described previously to explore your data. Then print an array that describes all the unique values for `Entity` for which we have data."
      ],
      "metadata": {
        "id": "9XVimqXpaZ3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "yY9sSyeSYu-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering your Data"
      ],
      "metadata": {
        "id": "C8zN4Vx-YScM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the datasets you work with get larger, it may become more important to filter your data so that you can focus on specific insights. Here are some basics for how to filter your dataframe:\n",
        "* Selecting Columns\n",
        "  * `df[\"column\"]`: Filter for a single column.\n",
        "  * `df[[\"col1\",\"col2\"]]`: Filter for multiple columns.\n",
        "* Filtering Rows (Basic Conditions)\n",
        "  * `df[df[\"column\"] == value]`: Row filter with one condition; filters for all rows in `column` that equal `value` (e.g., filter rows for a specific country like `\"United States\"`).\n",
        "  * `df[df[\"column\"] >= value]`: Inequality filter; filters for all rows in `column` (e.g., `\"Year\"`)that are greater than or equal to `value` (e.g., `1990`).\n",
        "* Filtering Rows with Multiple Conditions\n",
        "  * `df[(df[\"Entity\"] == \"United States\") & (df[\"Year\"] >= 1990)]`: The `&` operator (`AND`) means that both conditions must be true in order to be filtered for.\n",
        "  * `df[(df[\"Entity\"] == \"United States\") | (df[\"Entity\"] == \"Canada\")]`: The `|` operator (`OR`) means that either conditions may be true in order to be filtered for.\n",
        "* Filtering with Lists and Keywords\n",
        "  * `df[df[\"Entity\"].isin([\"Brazil\",\"China\",\"India\"])]`: Filters for multiple values (e.g., `[\"India\", \"China\", \"Brazil\"]`) in a specific column (e.g., `\"Entity\"`), using the attribute `.isin()`. If a specific value is not in the column, this value is ignored.\n",
        "  * `df[df[\"Entity\"].str.contains(\"United\")]`: Filters for partial or complete keyword matching (e.g., \"United\") in a specific column (e.g., \"Entity\"), using the attribute `.str.contains()`.\n",
        "* Filtering with Ranges and Between\n",
        "  * `df[df[\"Year\"].between(2015,2020)]`: Filters based on a numeric range (e.g., `2015,2020`) in a specific column (`\"Year\"`), using the attribute `.between()`. Note that this range is inclusive andâ€”in this caseâ€”would include rows for both `2015` and `2020`.\n",
        "* Filtering Missing Data\n",
        "  * `df[df[\"0-4 years\"].isna()]`: Filters for all rows in a specific colum (e.g., `\"0-4 years\"`) where there is no data, using the attribute `.isna()`.\n",
        "  * `df[df[\"0-4 years\"].notna()]`: Filters for all rows in a specific colum (e.g., `\"0-4 years\"`) where there is data, using the attribute `.notna()`."
      ],
      "metadata": {
        "id": "g4aMPKFdaG6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ðŸ’ª Practice: Filter for Specific Countries\n",
        "Filter the DataFrame `df` so that you compare the populations of `India` and `China` from `1980` to `2020`."
      ],
      "metadata": {
        "id": "r-Cfpn9FlQVY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UhWphIoDmW0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Locating Data"
      ],
      "metadata": {
        "id": "HdKUMoqbTo6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to locate a specific elements in your dataset, you can also use the `.iloc[]` and `.loc[]` commands. These commands have *very* similiar functions but different syntax, which can make them difficult to use.\n",
        "* Index by Position (`.iloc[]`): When you are using Row and Column Indicies\n",
        "  * `df.iloc[0]`: Returns the first row (Row Index `0`).\n",
        "  * `df.iloc[0:3]`: Returns rows that have Row Indicies `0`, `1`, and `2`.\n",
        "  * `df.iloc[5,2]`: Returns the value from  Row Index `5` and Column Index `2`.\n",
        "  * `df.iloc[[2,5]][[\"Year\"]]`: Returns Row Indicies `2` and `5` in the column `\"Year\"`.\n",
        "  * **Note about Slicing**: `iloc[start:stop]` slices the same way as Python, meaning that it will start with `start` and go uptoâ€”but not inlcudeâ€”`stop`.\n",
        "* Index by Label (`.loc[]`): When you are using Row Indicies Column Labels\n",
        "  * `df.loc[0]`: Returns the first row (Row Index `0`). Same as `df.iloc[0]`.\n",
        "  * `df.loc[0:3]`: Returns rows that have Row Indicies `0`, `1`, `2`, **and** `3`.\n",
        "  * `df.loc[5, \"Year\"]`: Returns the value from Row Index `5` and Column Label `\"Year\"`.\n",
        "  * `df.loc[:,[\"Entity\",\"Year\"]]`: Returns all rows but selected columns `\"Entity\"` and `\"Year\"`.\n",
        "  * `df.loc[df[\"Entity\"] == \"India\"]`: Returns all rows where the entry for a specific column `\"Entity\"` has a specific value `\"India\"`.\n",
        "  * **Note about Slicing**: `loc[start:stop]` slices differently from Python, it will start with `start` and go uptoâ€”**and inlcude**â€”`stop`."
      ],
      "metadata": {
        "id": "84yuLGL4GxE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ’ª Practice: Compare `iloc[]` and `loc[]`\n",
        "\n",
        "Obtain the same outputs by slicing `df` using both `iloc[]` and `loc[]`:\n",
        "* Slice `df` to output the rows 3, 4, and 5.\n",
        "* Slice `df` to output the value corresponding with `Afghanistan`, `Year`, and `25-64 years` (row `3`, column `6`).\n"
      ],
      "metadata": {
        "id": "QtFfKNtMXcbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Slice for rows 3, 4, 5\n"
      ],
      "metadata": {
        "id": "dtyyZz0PTLbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Slice for row 3, column 6\n"
      ],
      "metadata": {
        "id": "GzcUmHoJfFjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤  Data Wrangling"
      ],
      "metadata": {
        "id": "R6e2DY9rT_k7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data wrangling (also called \"data cleaning\" or \"data preprocessing\") is the process of transforming raw data into a clean, organized, and usable format for analysis or visualization."
      ],
      "metadata": {
        "id": "kbdt0sMeUBzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "KJ3BvB_xuDFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning your data is a critical first step in data wrangling, as real-world datasets are often messy, inconsistent, or incomplete. This process ensures that your data is accurate, standardized, and ready for analysis by addressing issues like inconsistent formatting, unexpected data types, and missing values. Two key parts of this process include Column Formatting, which involves cleaning and standardizing individual columns, and Handling Missing Data, which ensures gaps in your dataset don't lead to misleading results."
      ],
      "metadata": {
        "id": "OpOfVVHv3hS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Column Formatting"
      ],
      "metadata": {
        "id": "90wICYvBuHAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Column formatting involves cleaning and standardizing individual columns to make your dataset easier to work with. This includes changing data types, removing excess whitespace, renaming columns, rounding numeric values, and replacing inconsistent labels:\n",
        "\n",
        "* `df[\"col\"].astype(type)`:\tConvert data types (e.g., to `float`, `int`, `str`)\n",
        "* `df[\"col\"].str.strip()`: Removes whitespace from strings, including spaces (`\" \"`), tabs (`\"\\t\"`), and newlines(`\"\\n\"`).\n",
        "* `df.rename(columns={\"old1\": \"new1\",\"old2\":\"new2\"}, inplace=True)`: Renames columns in `df`. `inplace=True` means that these changes will be saved in the original dataframe `df`.\n",
        "* `df[\"Status\"].replace({\"N/A\": \"Unknown\", \"M\": \"Male\", \"F\": \"Female\"})`:\tReplace specific values in one or more columns.\n",
        "* `df[\"col\"].round(2)`:\tRounds numbers in a column to a fixed number of decimal places."
      ],
      "metadata": {
        "id": "oreVIuVQt3Bh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Missing Data"
      ],
      "metadata": {
        "id": "4hmWzuXYZzRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detection**: The first thing you'll want to do is detect whether you have missing data (oftentimes denoted as `NaN` or \"Not a Number\") and identify where that missing data is. The following commands can help you detect/locate missing data:\n",
        "* `df.replace(\"N/A\", np.nan)`: Replaces the string `\"N/A\"` with a `NaN`.\n",
        "* `df.isna().sum()`: Counts the missing values by column.\n",
        "* `df[df[\"col_name\"].isna()]`: Filters for rows in a specific column (`\"col_name\"`) where data is missing.\n",
        "* `df[df[\"col_name\"].isna()].index`: Returns the index labels where rows in a specific column (e.g., `\"col_name\"`) are missing data.\n",
        "* `df[df.isna().any(axis=1)].index`: Returns the index of all rows with at least one `NaN` value.\n",
        "* `NaN_id = list(zip(*np.where(df.isna())))`: Returns a list of `(row_index, col_index)` for each `NaN`."
      ],
      "metadata": {
        "id": "5pqTNYWXrKPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of Indexing NaN values\n",
        "NaN_id = list(zip(*np.where(df.isna())))\n",
        "\n",
        "# Printing rows and columns\n",
        "for row, col in NaN_id:\n",
        "    print(f\"Missing at row {row}, column '{df.columns[col]}'\")"
      ],
      "metadata": {
        "id": "_Fn682lvrK0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on how missing data is managed in the dataset, missing data could be represented as a `0` instead of `NaN`. If this is the case, there is an equivalent set of commands to identify where `0`'s appear in your dataset:\n",
        "\n",
        "* `(df == 0).sum()`: Counts the number of `0` values by column.\n",
        "* `df[df[\"col_name\"] == 0]`: Filters for rows in a specific column (`\"col_name\"`) where data is `0`.\n",
        "* `df[df[\"col_name\"] == 0].index`: Returns the index labels where rows in a specific column (e.g., `\"col_name\"`) where data is `0`.\n",
        "* `df[(df == 0).any(axis=1)].index`: Returns the index of all rows with at least one `0` value.\n",
        "* `Zero_id = list(zip(*np.where(df == 0)))`: Returns a list of `(row_index, col_index)` for each `0`."
      ],
      "metadata": {
        "id": "bVvEDzt-EP5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filling In**: Once you have located your missing data, you can fill in the missing entries using reasonable assumptions. To do this, we'll primarily use the command `.fillna()`:\n",
        "\n",
        "* `df_filled = df.fillna(0)`: Fills in `NaN`'s with `0`'s.\n",
        "* `df_filled = df[\"0-4 years\"].fillna(1000000)`: Fills in `NaN`'s in column `0-4 years` with a specific value of `1000000`.\n",
        "* `df_filled = df[\"0-4 years\"].fillna(df[\"0-4 years\"].mean())`: Fills in `NaN`'s in column `0-4 years` with the mean value from that column. Could also use `.median()` for median and `.mode()[0]` for first-most common value.\n",
        "* `df_filled = df.fillna(method=\"ffill\")`: \"Forward\" fills in `NaN` values using the previous row's value.\n",
        "* `df_filled = df.fillna(method=\"ffill\",limit=2)`: \"Forward\" fills in `NaN` values using the previous row's value, but only does this for the first 2 consecutive `NaN` values.\n",
        "* `df_filled = df.fillna(method=\"bfill\")`: \"Back\" fills in `NaN` values using the next row's value.\n",
        "* **Note**: It is generally a good idea to retain your original unmodified dataframe, saving any modifications in as a separate dataframe."
      ],
      "metadata": {
        "id": "_AAIbG6Ls_r-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, if you are working with an evenly-spaced time series, you could use the `.interpolate()` command to fill in your `NaN` values by interpolation:\n",
        "* `df_interp = df.interpolate(method=\"linear\")`: Fills in missing values by drawing a straight line between two known data points and estimating the values in between. This is the default method and would execute even if you put `df_interp = df.interpolate()`.\n",
        "* `df_interp = df.interpolate(method=\"polynomial\", order=2)`: Fits one single polynomial to all known points (e.g., order = 2 is quad, order = 3 is cubic)\n",
        "* `df_interp = df.interpolate(method=\"spline\", order=2)`: Fits piecewise polynomials between each pair of known points.\n",
        "* **Note**: These methods use the our dataframe's index to conduct the interpolation. While this works for an evenly-spaced time series, it may not work if you are working with data where this is not the case. You can still use interpolation, but it requires you setting your time series column (e.g., `\"Years\"`) as your index. This approach doesn't work well for our data because we have the same time series repeat for each `\"Entity\"` in our dataframe."
      ],
      "metadata": {
        "id": "SoUkK3oUh--e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# United States Data Before Intepolation\n",
        "df[df[\"Entity\"] == \"United States\"]"
      ],
      "metadata": {
        "id": "wIvnPHh4n4os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# United States Data After Interpolation\n",
        "df_interp = df.interpolate()\n",
        "df_interp[df_interp[\"Entity\"] == \"United States\"]"
      ],
      "metadata": {
        "id": "2QgJbRfys_UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deletion**: If you don't have a reasonable assumptions that you can make to fill in your data, it may be prudent to delete the data entirely to work with a complete subset of your data.  To do this, we'll primarily use the command `.dropna()`:\n",
        "* `df_rdrop = df.dropna()`: Removes all rows that contain `NaN` in any column.\n",
        "* `df_cdrop = df.dropna(axis=1)`: Removes all columns that contain any `NaN` in any row.\n",
        "* `df_subdrop = df.dropna(subset =[\"0-4 years\",\"5-14 years\"])`: Removes all rows that contain `NaN` in specified columns `0-4 years` or `5-14 years`.\n",
        "* `df.dropna(inplace=True)`: Removes all rows that contain `NaN` in any column. This will modify `df` in-place instead of returning a new one.\n",
        "* **Note**: Same as before, it is generally a good idea to retain your original unmodified dataframe, saving any modifications in as a separate dataframe.\n",
        "* `df.drop_duplicates()`: Although not using `.dropna`, this command removes duplicate rows."
      ],
      "metadata": {
        "id": "0j152yavs0xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# United States Data Before Deletion\n",
        "df[df[\"Entity\"] == \"United States\"]"
      ],
      "metadata": {
        "id": "X09Y6tFwtMF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# United States Data After Deletion\n",
        "df_drop = df.dropna()\n",
        "df_drop[df_drop[\"Entity\"] == \"United States\"]"
      ],
      "metadata": {
        "id": "oYrUlrD2oStC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ’ª Practice: Clean US Population Data\n",
        "\n",
        "Clean the dataframe `df_US` by converting all missing data to `NaN` and then linearly interpolating to fill in this data. Note that multiple conventions to indicate missing data are being used."
      ],
      "metadata": {
        "id": "8s4LZVmF1rtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f\n",
        "\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Population DataFrame\n",
        "url = \"https://raw.githubusercontent.com/cdefinnda/Python-Intro/main/Datasets/Population_incomplete_US2.csv\"\n",
        "df2 = pd.read_csv(url)\n",
        "\n",
        "# Create New DataFrame that isolates US Data: df_US\n",
        "df_US = df2[df2[\"Entity\"] == \"United States\"]\n",
        "\n",
        "# Begin Cleaning US Data Here:\n"
      ],
      "metadata": {
        "id": "qSvPSXgW2P15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "WV59fkU6Hjth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have cleaned your dataset, the next step is data processing â€” the stage where you begin to shape your data into a form that reveals insights. This includes feature engineering, where you create new columns or metrics from existing data (such as calculating growth rates or per-capita values), and data integration, where you combine multiple datasets to enrich your analysis. These processes help turn raw information into meaningful variables that better support visualizations, modeling, or decision-making."
      ],
      "metadata": {
        "id": "-cijbKkDSFRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering"
      ],
      "metadata": {
        "id": "UWhgZRzMux57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature engineering is the process of creating new columnsâ€”or featuresâ€”from existing data to make patterns more visible, improve interpretability, or support more effective analysis. In this section, you'll learn how to use **math operations between columns** to calculate sums, ratios, averages, and differences, which are often more meaningful than raw values. You'll also **derive analytical metrics** such as log transformations, percentages, and z-scores that help normalize or rescale data, making it easier to compare across variables or identify outliers.\n",
        "\n",
        "**Math Operations between Columns**\n",
        "* `df[\"sum\"] = df[\"A\"] + df[\"B\"]`: Creates a new column `sum` that contains the row-wise sum of column `A` and column `B`. Also works for subtraction `-`.\n",
        "* `df[\"ratio\"] = df[\"A\"] / df[\"B\"]`: Creates a new column `ratio` that contains the row-wise division of column `A` by column `B`. Also works for multiplication `*`.\n",
        "* `df[\"avg\"] = df[[\"A\",\"B\"]].mean(axis=1)`: Creates a new column `avg` that contains the row-wise average of column `A` and column `B`.\n",
        "* `df[\"max\"] = df[[\"A\", \"B\"]].max(axis=1)`: Creates a new column `max` that contains the row-wise maximum between column `A` and column `B`. Also works for `.min(axis=1)`.\n",
        "* `df[\"avg_if\"] = df[[\"A\",\"B\"]].mean(axis=1).where(df[\"A\"] > df[\"B\"], df[\"B\"])`: Creates a new column `avg_if` that contains the row-wise average of column `A` and column `B` if the row-wise value in column `A` is greater than the row-wise value in column `B`. If this condition is not met, it will output the row-wise value in column `B`.\n",
        "\n",
        "**Deriving Analytical Metrics**\n",
        "* `df[\"percent_change\"] = df[\"A\"].pct_change()`: Creates a new column `percent_change` that contains the percent change between subsequent row values in column `A`.\n",
        "* `df[\"cumulative\"] = df[\"A\"].cumsum()`: Creates a new column `cumulative` that contains the cumulative sum of all preceding rows in column `A`. Also works for the cumulative product: `.cumprod()`.\n",
        "* `df[log_col] = np.log(df[\"A\"])`: Creates a new column `log_col` that contains the natural log (base $e$) of the row-wise values in column `A`. Also works for `np.log10()` and `np.log2()`.\n",
        "* `df[\"percent\"] = df[\"A\"] / df[\"A\"].sum()`: Creates a new column `percent` that contains the percent of the row-wise value in column `A` relative to the sum of all values in the entire column `A`.\n",
        "* `df[\"zscore\"] = (df[\"A\"] - df[\"A\"].mean()) / df[\"A\"].std()`: Creates a new column `zscore` that contains the z-score of the row-wise value in column `A` relative to the entire column `A`."
      ],
      "metadata": {
        "id": "xMFsM2yZS_03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ’ª Practice: Convert to Percentages\n",
        "\n",
        "Create a new column in `df_pop` called `total` that takes the sum across all age categories by year. Then create a new column called `percent_0-4` that calculates the percent of people who are 0 - 4 years in age relative to the total population by year. Repeat this for all other age groups."
      ],
      "metadata": {
        "id": "R_yoG8Fdap3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f\n",
        "\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Population DataFrame\n",
        "url = \"https://raw.githubusercontent.com/cdefinnda/Python-Intro/main/Datasets/Population.csv\"\n",
        "df_pop = pd.read_csv(url)\n",
        "\n",
        "# Begin Feature Engineering Here:\n"
      ],
      "metadata": {
        "id": "kNJ9Wgrru39m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Integration"
      ],
      "metadata": {
        "id": "D5949RAWrc-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data integration is the process of combining information from multiple sources into a single, unified dataset. Once you've created meaningful features, integrating datasetsâ€”like merging population data with other relavant datasetsâ€”adds valuable context and supports more powerful analysis. In Pandas, data integration can be done in several ways, including merging on a common key, joining on index values, or concatenating rows or columns depending on how your data is structured:\n",
        "\n",
        "**Merging Datasets**\n",
        "* `pd.merge(df1,df2,on=\"key\")`: Merges `df1` and `df2` for matching values in the `\"key\"` column. If you want to match based on multiple `\"key\"` columns, you can use `on=[\"key1\",\"key2\"]` instead. Note that this requires some degree of data cleaning in order to ensure your `\"key\"` columns have the same exact names.\n",
        "* `pd.merge(df1,df2,left_on=[\"Entity\", \"Year\"], right_on[\"region\", \"yr\"])`: Merges `df1` and `df2` for matching values in different `\"key\"` columns. In this case, the `\"key\"` columns for `df1` are `\"Entity\"` and `\"Year\"` and the `\"key\"` columns for `df2` are `\"region\"` and `\"yr\"`.\n",
        "* `pd.merge(df1,df2,on=\"key\", how=\"arg\")`: Specifies how the merge will take place:\n",
        "  * `\"arg\" = \"inner\"` (default): Only keep rows where the key exists in **both** DataFrames\n",
        "  * `\"arg\" = \"left\"`: Keep all rows from `df1`, add matching values from `df2`\n",
        "  * `\"arg\" = \"right\"`: Keep all rows from `df2`, adding matching values from `df1`\n",
        "  * `\"arg\" = \"outer\"`: Keep all rows from both DataFrames; fill missing values with `NaN`.\n",
        "\n",
        "**Joining on Index**\n",
        "* `df1.join(df2)`: Combines `df2` to `df1` based on their respective indicies. This method is faster, but requires that all row entries are aligned ahead of time.\n",
        "\n",
        "**Concatenating Datasets**\n",
        "* `df3 = pd.concat([df1, df2], axis=0)`: Stacks rows vertically, starting with `df1` and followed by `df2`.\n",
        "* `df3 = pd.concat([df1, df2], axis=1)`: Combines columns side-byside, starting with `df1` and followed by `df2`."
      ],
      "metadata": {
        "id": "_eGPRX-Xe9KG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ’ª Practice: Merge Population and Population Changes Datasets\n",
        "\n",
        "Merge the `df_pop` and `df_change` datasets using the pd.merge function. Note that the column names in each dataset are different. In terms of how, use the `\"outer\"` method and see if you can identify where data is missing."
      ],
      "metadata": {
        "id": "4nNjFRR7kCh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f\n",
        "\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Population DataFrame\n",
        "url_pop = \"https://raw.githubusercontent.com/cdefinnda/Python-Intro/main/Datasets/Population.csv\"\n",
        "df_pop = pd.read_csv(url_pop)\n",
        "url_change = \"https://raw.githubusercontent.com/cdefinnda/Python-Intro/main/Datasets/Population_Changes.csv\"\n",
        "df_change = pd.read_csv(url_change)\n",
        "\n",
        "# Begin Merge Here:\n"
      ],
      "metadata": {
        "id": "Fk7D3epfagYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualization"
      ],
      "metadata": {
        "id": "r7J__t30o8RK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data visualization is a powerful way to explore, understand, and communicate patterns in your data. After processing and integrating your dataset, creating plots helps you identify trends, compare values across groups, and uncover relationships that might not be obvious in raw numbers. Well-designed visualizations not only support analysis, but also make your findings more accessible to others."
      ],
      "metadata": {
        "id": "X_gxKS-Rm5la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To practice, we'll use the same population dataset from [Our World in Data](https://ourworldindata.org/population-growth):"
      ],
      "metadata": {
        "id": "H1QJZF64pIhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f\n",
        "\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Population DataFrame\n",
        "url_pop = \"https://raw.githubusercontent.com/cdefinnda/Python-Intro/main/Datasets/Population.csv\"\n",
        "df_pop = pd.read_csv(url_pop)"
      ],
      "metadata": {
        "id": "tkQY3Ok4pGDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make Data Visualization easier, Pandas actually has a built-in plotting function that allows you to visualize your data more easily:\n",
        "\n",
        "* `df.plot(x = \"Year\", y = \"Population\")`: Produces a line plot (default) of `\"Population\"` as a function of `\"Year\"`.\n",
        "* `df.plot(x = \"Year\", y = [\"Population\",\"Births\", \"Deaths\"])`: Produces multiple line plots of `\"Population\"`, `\"Births\"`, and `\"Deaths\"` as a function of `\"Year\"`.\n",
        "* `df.plot(kind=\"bar\", x = \"Country\", y = \"Population\")`: Produces a bar plot of `\"Population\"` as a function of `\"Country\"`.\n",
        "* `df.plot(kind=\"hist\", bins=10)`: Produces a histogram of data (e.g., grades on an exam) in 10 bins.\n",
        "* `df.plot(kind=\"scatter\", x = \"Births\", y = \"Deaths\")`: Produces a scatter plot of `\"Deaths\"` as a function of `\"Births\"`.\n",
        "* `df.boxplot(column=\"Age\", by=\"Region\")`: Produces a box plot that visualizes the spread and outliers of `\"Age\"` by category `\"Region\"`.\n",
        "\n",
        "\n",
        "\n",
        "Similar to Matplotlib, you can also specify the `title`, `xlabel`, `ylabel`, `xlim`/`ylim`, `grid`, `legend`, `figsize`, `style`, `color`, and `marker`. Lastly, for more complicated datasets, you may need to filter your dataset prior plotting."
      ],
      "metadata": {
        "id": "GxfDPY3CnHH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter Data for US\n",
        "df_us = df_pop[df_pop[\"Entity\"] == \"United States\"]\n",
        "\n",
        "# Line Plot\n",
        "df_us.plot(x=\"Year\", y=[\"0-4 years\",\"5-14 years\",\"15-24 years\",\"25-64 years\",\"65+ years\"], title=\"Population Over Time (U.S.)\", ylabel=\"Population\")"
      ],
      "metadata": {
        "id": "Y0ZEU9OqrKBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the built-in plotting functions in Pandas, you can also use `Matplotlib` to customize your plots further"
      ],
      "metadata": {
        "id": "Lq-Ryyl1aJQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(df_us[\"Year\"], df_us[\"0-4 years\"], label = \"0-4 years\")\n",
        "plt.plot(df_us[\"Year\"], df_us[\"5-14 years\"], label = \"5-14 years\")\n",
        "plt.plot(df_us[\"Year\"], df_us[\"15-24 years\"], label = \"15-24 years\")\n",
        "plt.plot(df_us[\"Year\"], df_us[\"25-64 years\"], label = \"25-64 years\")\n",
        "plt.plot(df_us[\"Year\"], df_us[\"65+ years\"], label = \"65+ years\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Population\")\n",
        "plt.title(\"Population Over Time (U.S.)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6TnZLBYZto5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ’ª Practice: Visualizing Internet Usage by Country\n",
        "Use `Matplotlib` to plot the % of population using internet:\n",
        "* Start by renaming the column `\"Individuals using the Internet (% of population)\"` to `\"% Internet Usage\"`.\n",
        "* Plot `\"% Internet Usage\"` for `United States`, `China`, `India` from `1990` to `2022`.\n",
        "* Make sure to include a plot title, x-label, y-axis label, and a legend.\n"
      ],
      "metadata": {
        "id": "uTefk12xgQNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f\n",
        "\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Internet DataFrame\n",
        "url_internet = \"https://raw.githubusercontent.com/cdefinnda/Python-Intro/main/Datasets/Internet.csv\"\n",
        "df_internet = pd.read_csv(url_internet)\n",
        "\n",
        "# Continue code here..."
      ],
      "metadata": {
        "id": "_h470kmigWZ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": "40",
        "lenType": 16,
        "lenVar": "100"
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MtO1z0WdGzyv",
        "R6e2DY9rT_k7"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}